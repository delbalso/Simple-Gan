{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tm2CQN_Cpwj0"
   },
   "source": [
    "# Basic Generative Adversarial Network to generate MNIST images\n",
    "\n",
    "This code creates two networks: a generator and a discriminator. The discriminator tries to determine if it's input is coming from real MNIST data or not and the generator tries to fool the discriminator. The generator (which is the network that is making the images seen below) never actually sees data from the training set, it just connects to the discriminator and has a loss function based on the discriminator's output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pylab as pl\n",
    "import time\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PIL.Image as im\n",
    "\n",
    "def save_image(image):\n",
    "    im.fromarray(np.uint8(pl.cm.Greys(image)*255)).convert('RGB').save(\"result.jpeg\")\n",
    "    \n",
    "pl.rcParams[\"figure.figsize\"] = 15,15\n",
    "\n",
    "def plot_images(images):\n",
    "    images = np.squeeze(images)\n",
    "    edge_size = np.sqrt(images.shape[0])\n",
    "    new_row = []\n",
    "    for i in xrange(images.shape[0]):\n",
    "        new_row.append(images[i])\n",
    "        if (i + 1) % edge_size == 0:\n",
    "            if i < edge_size: #full_image doesn't exist yet\n",
    "                full_image = np.hstack(new_row)\n",
    "            else:\n",
    "                full_image = np.vstack([full_image,np.hstack(new_row)])\n",
    "            new_row = []\n",
    "    save_image(full_image)\n",
    "    pl.imshow(full_image,cmap=\"gray_r\")\n",
    "    \n",
    "    display.display(pl.gcf())\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.scalar_summary('mean/' + name, mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "    tf.scalar_summary('sttdev/' + name, stddev)\n",
    "    tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "    tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "    tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv(input_data, shape, k=2, name=\"conv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        filter = tf.get_variable(\"filter\", shape, initializer=tf.truncated_normal_initializer(stddev=1))\n",
    "        b = tf.get_variable(\"b\", [shape[3]], initializer=tf.constant_initializer())\n",
    "        return tf.nn.conv2d(input_data, filter, [1, k, k, 1], padding='SAME')+b\n",
    "\n",
    "\n",
    "def fully_connected(input_data, shape, use_bias = True, stddev = 0.02, name=\"fully_connected\"):\n",
    "    assert input_data.get_shape()[1] == shape[0], \"input shape = {0}, w shape = {1}\".format(input_data.get_shape(),shape)\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable(\"w\", shape, initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        if use_bias:\n",
    "            b = tf.get_variable(\"b\", [shape[1]], initializer=tf.constant_initializer())\n",
    "            return tf.matmul(input_data, w) + b\n",
    "        else:\n",
    "            return tf.matmul(input_data, w)\n",
    "\n",
    "def minibatch(input, num_kernels=5, kernel_dim=3, name=\"minibatch\"):\n",
    "    x = fully_connected(input, [input.get_shape()[1], num_kernels * kernel_dim], use_bias=False, stddev = 10, name=name)\n",
    "    activation = tf.reshape(x, (-1, num_kernels, kernel_dim))\n",
    "    diffs = tf.expand_dims(activation, 3) - \\\n",
    "        tf.expand_dims(tf.transpose(activation, [1, 2, 0]), 0)\n",
    "    abs_diffs = tf.reduce_sum(tf.abs(diffs), 2)\n",
    "    minibatch_features = tf.reduce_sum(tf.exp(-abs_diffs), 2)\n",
    "    return tf.concat(1, [minibatch_features, input])\n",
    "\n",
    "def discriminator(data, reuse=True):\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        channels = 512\n",
    "        d_l1 = conv(data, shape=[4, 4, num_channels, channels/4], k=2, name=\"layer1\")\n",
    "        d_l2 = batch_norm(conv(tf.nn.relu6(d_l1), shape=[4, 4, channels/4, channels/2], k=2,name=\"layer2\"), channels/2, name = \"layer2\")\n",
    "        d_l3 = batch_norm(conv(tf.nn.relu6(d_l2), shape=[4, 4, channels/2, channels], k=2, name=\"layer3\"), channels, name = \"layer3\")\n",
    "        fm3_shape = 4*4*channels\n",
    "        d_l3_reshaped = tf.reshape(d_l3, [tf.shape(d_l3)[0],fm3_shape])\n",
    "        #data_reshaped = tf.reshape(data, [64,28*28])\n",
    "        \n",
    "        mb = minibatch(d_l3_reshaped, name=\"layer3_minibatch\")\n",
    "        print(\"mb shape {0}\".format(mb.get_shape()))\n",
    "        d_l4 = fully_connected(tf.nn.relu6(mb), shape=[mb.get_shape()[1],2], name=\"layer4\")\n",
    "        return tf.Print(tf.nn.sigmoid(d_l4),[mb[:5,0]], message = \"MB's Value: \")\n",
    "        #return tf.nn.sigmoid(d_l4)\n",
    "    \n",
    "def deconv2d(input_data, filter_shape, output_shape, padding = 'SAME', k = 2, name=\"deconv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        filter = tf.get_variable(\"filter\", filter_shape, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        return tf.nn.conv2d_transpose(value=input_data,\n",
    "                                     filter=filter, \n",
    "                                     output_shape=output_shape,\n",
    "                                     strides=[1,k,k,1],\n",
    "                                     padding=padding, name=\"feature_maps\")\n",
    "    \n",
    "def batch_norm(input_data, size, conv = True, name=\"batch_norm\"):\n",
    "    with tf.variable_scope(name):\n",
    "        offset = tf.get_variable(\"offset\", [size], initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        scale = tf.get_variable(\"scale\", [size], initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "        if conv:\n",
    "            mean, variance = tf.nn.moments(input_data,axes=[0,1,2])\n",
    "        else:\n",
    "            mean, variance = tf.nn.moments(input_data,axes=[0])\n",
    "        return tf.nn.batch_normalization(input_data, mean, variance, offset, scale, variance_epsilon=1e-5)\n",
    "    \n",
    "def generator(noise, reuse=True):\n",
    "    num_examples = tf.shape(noise)[0]\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            \n",
    "        channels = 512\n",
    "\n",
    "        l0 = fully_connected(noise, shape=[noise.get_shape()[1],4*4*channels], name=\"layer0\")\n",
    "        l0_sq = tf.reshape(l0, [num_examples,4,4,channels])\n",
    "        layer0 =  tf.nn.relu(batch_norm(l0_sq,channels, name = \"layer0\"))\n",
    "        \n",
    "        \n",
    "        fm1 = deconv2d(layer0, # 64,8,8,1\n",
    "                      filter_shape= [4,4,channels/2,channels], \n",
    "                      output_shape=[num_examples,7,7,channels/2],\n",
    "                      padding = \"VALID\",\n",
    "                      k=1,\n",
    "                      name = \"layer1\")\n",
    "        layer1 = tf.nn.relu(batch_norm(fm1,channels/2, name = \"layer1\"))\n",
    "        \n",
    "        fm2 = deconv2d(layer1,\n",
    "                      filter_shape= [4,4,channels/4,channels/2],\n",
    "                      output_shape=[num_examples,14,14,channels/4],\n",
    "                      name = \"layer2\")\n",
    "        layer2 = tf.nn.relu(batch_norm(fm2,channels/4, name = \"layer2\"))\n",
    "\n",
    "        fm3 = deconv2d(layer2,\n",
    "                      filter_shape=[4,4,1,channels/4],\n",
    "                      output_shape=[num_examples,28,28,1],\n",
    "                      name = \"layer3\")\n",
    "\n",
    "        layer3 = tf.nn.tanh(fm3)/2+.5\n",
    "        output = layer3\n",
    "        #tf.reshape(layer4,[-1,image_size, image_size,1])\n",
    "        print(\"layer1 shape= {0}\".format(layer1.get_shape()))\n",
    "        print(\"layer2 shape= {0}\".format(layer2.get_shape()))\n",
    "        print(\"layer3 shape= {0}\".format(layer3.get_shape()))\n",
    "#        print(\"layer4 shape= {0}\".format(layer4.get_shape()))\n",
    "        print(\"output shape= {0}\".format(output.get_shape()))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "image_size = 28\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "random_vector_size = 100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels)) #images\n",
    "    tf_train_random = tf.placeholder(tf.float32, shape=(batch_size, random_vector_size))\n",
    "\n",
    "    # Variables for generator\n",
    "    hidden_layer1_size = 50\n",
    "    with tf.name_scope(\"normal_generator\"):\n",
    "        glayer1_weights = tf.Variable(tf.truncated_normal([random_vector_size,hidden_layer1_size], stddev=0.1), name=\"w1\")\n",
    "        glayer2_weights = tf.Variable(tf.truncated_normal([hidden_layer1_size,image_size * image_size], stddev=0.1), name=\"w2\")\n",
    "        glayer1_biases = tf.Variable(tf.zeros([hidden_layer1_size]), name=\"b1\")\n",
    "        glayer2_biases = tf.Variable(tf.zeros([image_size * image_size]), name=\"b2\")\n",
    "\n",
    "    \n",
    "    #Generator Loss\n",
    "    sample_points = tf.constant(np.random.uniform(0,1,(batch_size,random_vector_size)).astype(np.float32))\n",
    "    debug_image = generator(sample_points, reuse=False)\n",
    "    generated_image = generator(tf_train_random)\n",
    "    \n",
    "    generator_logits = discriminator(generated_image, reuse=False)\n",
    "    generator_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(generator_logits,tf.tile(tf.constant([[.1,.9]],dtype=tf.float32),[batch_size,1])))\n",
    "    \n",
    "    generator_loss2 = tf.nn.l2_loss(generated_image-tf_train_dataset)\n",
    "    \n",
    "\n",
    "    # Generator Optimizer\n",
    "    generator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\")\n",
    "    generator_learnrate = tf.train.exponential_decay(0.001, global_step, 10000, 0.96, staircase=True)\n",
    "    g_opt = tf.train.AdamOptimizer(generator_learnrate)\n",
    "    g_gradients = g_opt.compute_gradients(generator_loss, var_list=generator_variables)\n",
    "    g_apply = g_opt.apply_gradients(g_gradients, global_step=global_step)\n",
    "\n",
    "    generator_optimizer2 = tf.train.AdamOptimizer(.003).minimize(generator_loss2, var_list=generator_variables, global_step=global_step)  \n",
    "    \n",
    "    # Discriminator Loss\n",
    "    classifier_real_logits = discriminator(tf_train_dataset)\n",
    "    print(tf_train_dataset.get_shape())\n",
    "    print(classifier_real_logits.get_shape())\n",
    "    print(generator_logits.get_shape())\n",
    "    classifier_real_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(classifier_real_logits,tf.tile(tf.constant([[.1,.9]],dtype=tf.float32),[batch_size,1])))\n",
    "\n",
    "    classifier_fake_logits = generator_logits\n",
    "    classifier_fake_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(classifier_fake_logits,tf.tile(tf.constant([[.9,.1]],dtype=tf.float32),[batch_size,1])))\n",
    "    classifier_acc = tf.reduce_sum(tf.pack([tf.round(tf.nn.softmax(classifier_real_logits))[:,1],\n",
    "                                            tf.round(tf.nn.softmax(classifier_fake_logits))[:,0]]))/(batch_size*2)\n",
    "    variable_summaries(classifier_acc, \"discriminator/accuracy\")\n",
    "    print (\"acc shape {0}\".format(classifier_acc.get_shape()))\n",
    "    #a = 0#tf.reduce_sum(tf.pack([tf.round(tf.nn.softmax(classifier_real_logits))[:,1],\n",
    "                              #tf.round(tf.nn.softmax(classifier_fake_logits))[:,0]]))\n",
    "\n",
    " \n",
    "    classifier_loss = classifier_real_loss + classifier_fake_loss\n",
    "\n",
    "    # Discriminator Optimizer.\n",
    "    classifier_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "    classifier_learnrate = tf.train.exponential_decay(0.0004, global_step, 10000, 0.96, staircase=True)\n",
    "    classifier_optimizer = tf.train.AdamOptimizer(classifier_learnrate).minimize(classifier_loss, var_list=classifier_variables, global_step=global_step)\n",
    "    d_opt = tf.train.AdamOptimizer(classifier_learnrate)\n",
    "    d_gradients = d_opt.compute_gradients(classifier_loss, classifier_variables)\n",
    "    d_apply = d_opt.apply_gradients(d_gradients, global_step=global_step)\n",
    "    \n",
    "    # Log variables\n",
    "    for var in classifier_variables:\n",
    "        print(var.name)\n",
    "        variable_summaries(var, var.name)\n",
    "    for var in generator_variables:\n",
    "        variable_summaries(var, var.name)\n",
    "    variable_summaries(classifier_real_logits, \"discriminator/real_logits\")\n",
    "    variable_summaries(classifier_fake_logits, \"discriminator/fake_logits\")\n",
    "    variable_summaries(generator_logits, \"generator/logits\")\n",
    "        \n",
    "        \n",
    "    check = tf.add_check_numerics_ops()\n",
    "    #for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \".*filter\"):\n",
    "        #print (\"Image variable: {0}\".format(var.name))\n",
    "        #tf.summary.tensor_summary(var.name + '_tensor',var)\n",
    "    merged = tf.merge_all_summaries()\n",
    "    #p0 = tdb.plot_op(viz.viz_conv_weights,inputs=[tf.get_default_graph().get_tensor_by_name(\"generator/layer1/filter:0\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 70000\n",
    "step = 0\n",
    "updated_generator=True\n",
    "fake_data, updates = {}, []\n",
    "cr_loss, cf_loss, g_loss, d_acc = .5, .5, .5, .5\n",
    "#batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "#batch_data = np.tile(batch_data[1], (batch_size,1))\n",
    "#batch_data = batch_data.reshape([batch_size,28,28,1])\n",
    "#plot_images(batch_data)\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    train_writer = tf.train.SummaryWriter('/tmp/train', session.graph)\n",
    "    print('Initialized')\n",
    "\n",
    "    for step in xrange(num_steps):\n",
    "        #feed_dict = {tf_train_dataset : batch_data, \n",
    "        #             tf_train_random: np.random.random((batch_size,random_vector_size)).astype(np.float32)}\n",
    "        #image,_ = session.run([generated_image, generator_optimizer2], feed_dict=feed_dict)\n",
    "        #if (step%10==0):\n",
    "        #    plot_images(image)\n",
    "        #continue\n",
    "\n",
    "        # prepare batch of training data\n",
    "        batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "        batch_data = batch_data.reshape([-1,28,28,1])\n",
    "        feed_dict = {tf_train_dataset : batch_data, \n",
    "                     tf_train_random: np.random.random((batch_size,random_vector_size)).astype(np.float32)}\n",
    "\n",
    "        _ = session.run([check], feed_dict=feed_dict)\n",
    "\n",
    "        #if (d_acc<.8) and step != 50 or step<30:\n",
    "        updates.append(\"_\")\n",
    "        d_grad, _, cr_loss, cf_loss, d_acc = session.run([d_gradients, \n",
    "                                               d_apply, \n",
    "                                               classifier_real_loss,\n",
    "                                               classifier_fake_loss,\n",
    "                                               classifier_acc\n",
    "                                              ], feed_dict=feed_dict)\n",
    "    \n",
    "        for i in xrange(int((d_acc*1.5+1)**2)):\n",
    "            updates.append(\"G\")\n",
    "            g_grad, _, g_loss, d_acc = session.run([g_gradients, g_apply, generator_loss, classifier_acc], feed_dict=feed_dict)\n",
    "            updated_generator=True\n",
    "\n",
    "        summary, gs = session.run([merged, global_step], feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, gs)\n",
    "        #redirect.start()\n",
    "        #print (redirect.stop())\n",
    "        \n",
    "        if (step % 10 == 0):\n",
    "            print(\"{0}  Generator Loss: {1:.3f}, Classifier loss: {2:.3f}, Real: {3:.3f}, Fake: {4:.3f}, Acc: {5:.2f}\".format(''.join(updates), g_loss, cr_loss + cf_loss, cr_loss, cf_loss, d_acc))\n",
    "            updates=[]\n",
    "        if (step % 10 == 0):\n",
    "            \n",
    "            images = session.run([generated_image], feed_dict=feed_dict)\n",
    "            assert (len(d_grad)==len(classifier_variables))\n",
    "            if step>=2:\n",
    "                assert (len(g_grad)==len(generator_variables))\n",
    "                for i in xrange(len(generator_variables)):\n",
    "                    g = g_grad[i][0]\n",
    "                    print (\"{0}'s gradients have {1} ({2:.2f}%) zeros and mean {3}\".format(generator_variables[i].name,g.size-np.count_nonzero(g), (g.size-np.count_nonzero(g)+0.0)/g.size*100, np.mean(g)))\n",
    "            for i in xrange(len(classifier_variables)):\n",
    "                g = d_grad[i][0]\n",
    "                print (\"{0}'s gradients have {1} ({2:.2f}%) zeros and mean {3}\".format(classifier_variables[i].name,g.size-np.count_nonzero(g), (g.size-np.count_nonzero(g)+0.0)/g.size*100, np.mean(g)))\n",
    "            print(tf.get_default_graph().get_tensor_by_name(\"discriminator/layer3_minibatch/w:0\").eval()[0])\n",
    "            \n",
    "            \n",
    "            print(\"Step {0}\".format(step))\n",
    "            #print(\"Classifier loss: {0}, Real: {1}, Fake: {2}\".format(cr_loss + cf_loss, cr_loss, cf_loss))\n",
    "            #print(\"Generator Loss: {0}\".format(g_loss))\n",
    "            clr, glr = session.run([classifier_learnrate,generator_learnrate])\n",
    "            print (\"Classifier learn rate: {0:.8f}, Generator learn rate: {1:.8f}\".format(clr,glr))\n",
    "            \n",
    "            if updated_generator:\n",
    "                plot_images(session.run([debug_image]))\n",
    "                updated_generator=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fake_data = {}\n",
    "#generated_images = {}\n",
    "#for i in xrange(10):\n",
    "#    fake_data[i] = np.random.random((batch_size,random_vector_size)).astype(np.float32)\n",
    "#    feed_dict = {tf_train_random: fake_data[i]}\n",
    "#    generated_images[i] = session.run([generated_image], feed_dict=feed_dict)\n",
    "#    plot_images(generated_images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
