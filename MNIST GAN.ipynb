{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tm2CQN_Cpwj0"
   },
   "source": [
    "# Basic Generative Adversarial Network to generate MNIST images\n",
    "\n",
    "This code creates two networks: a generator and a discriminator. The discriminator tries to determine if it's input is coming from real MNIST data or not and the generator tries to fool the discriminator. The generator (which is the network that is making the images seen below) never actually sees data from the training set, it just connects to the discriminator and has a loss function based on the discriminator's output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import range\n",
    "import pylab as pl\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.rcParams[\"figure.figsize\"] = 15,15\n",
    "\n",
    "def plot_images(session,image_graph):\n",
    "    images = session.run([image_graph])\n",
    "    images = np.squeeze(images)\n",
    "    edge_size = np.sqrt(images.shape[0])\n",
    "    new_row = []\n",
    "    for i in xrange(images.shape[0]):\n",
    "        new_row.append(images[i])\n",
    "        if (i + 1) % edge_size == 0:\n",
    "            if i < edge_size: #full_image doesn't exist yet\n",
    "                full_image = np.hstack(new_row)\n",
    "            else:\n",
    "                full_image = np.vstack([full_image,np.hstack(new_row)])\n",
    "            new_row = []\n",
    "    pl.imshow(full_image,cmap=\"gray\")\n",
    "    display.display(pl.gcf())\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('/tmp/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.scalar_summary('mean/' + name, mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "    tf.scalar_summary('sttdev/' + name, stddev)\n",
    "    tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "    tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "    tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "random_vector_size = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels)) #images\n",
    "    tf_train_random = tf.placeholder(tf.float32, shape=(batch_size, random_vector_size)) #numerals\n",
    "\n",
    "    # Variables for generator\n",
    "    hidden_layer1_size = 50\n",
    "    with tf.name_scope(\"normal_generator\"):\n",
    "        glayer1_weights = tf.Variable(tf.truncated_normal([random_vector_size,hidden_layer1_size], stddev=0.1), name=\"w1\")\n",
    "        glayer2_weights = tf.Variable(tf.truncated_normal([hidden_layer1_size,image_size * image_size], stddev=0.1), name=\"w2\")\n",
    "        glayer1_biases = tf.Variable(tf.zeros([hidden_layer1_size]), name=\"b1\")\n",
    "        glayer2_biases = tf.Variable(tf.zeros([image_size * image_size]), name=\"b2\")\n",
    "    \n",
    "    # Variables for deconv generator\n",
    "    with tf.name_scope(\"generator\"):\n",
    "        deconv_filter_1 = tf.Variable(tf.truncated_normal([5,5,256,1], stddev=0.1), name=\"filters/1\")\n",
    "        deconv_filter_2 = tf.Variable(tf.truncated_normal([8,8,64,256], stddev=0.1), name=\"filters/2\")\n",
    "        deconv_filter_3 = tf.Variable(tf.truncated_normal([10,10,1,64], stddev=0.1), name=\"filters/3\")\n",
    "        deconv_bias_1 = tf.Variable(tf.zeros([256]), name=\"biases1\")\n",
    "        deconv_bias_2 = tf.Variable(tf.zeros([64]), name=\"biases2\")\n",
    "        deconv_bias_3 = tf.Variable(tf.zeros([1]), name=\"biases3\")\n",
    "\n",
    "    # Variables for classifier\n",
    "    with tf.name_scope(\"discriminator\"):\n",
    "        layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, num_channels, depth], stddev=0.1), name=\"w1\")\n",
    "        layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, depth, depth], stddev=0.1), name=\"w2\")\n",
    "        layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "          [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1), name=\"w3\")\n",
    "        layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "          [num_hidden, 1], stddev=0.1), name=\"w4\")\n",
    "        layer1_biases = tf.Variable(tf.zeros([depth]), name=\"b1\")\n",
    "        layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]), name=\"b2\")\n",
    "        layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name=\"b3\")\n",
    "        layer4_biases = tf.Variable(tf.constant(1.0, shape=[1]), name=\"b4\")\n",
    "\n",
    "    # Model.\n",
    "    def classifier_model(data):\n",
    "        with graph.as_default():\n",
    "            with tf.name_scope(\"discriminator\"):\n",
    "                conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME', name=\"layer1\")\n",
    "                hidden = tf.nn.relu(conv + layer1_biases)\n",
    "                conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME', name=\"layer2\")\n",
    "                hidden = tf.nn.relu(conv + layer2_biases)\n",
    "                shape = tf.shape(hidden)\n",
    "                reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases, name=\"layer3\")\n",
    "                return (tf.matmul(hidden, layer4_weights) + layer4_biases)\n",
    "\n",
    "    #def generator_model(labels):\n",
    "    #    hidden1 = tf.nn.relu(tf.matmul(labels, glayer1_weights) + glayer1_biases)\n",
    "    #    hidden2 = tf.sigmoid(tf.matmul(hidden1, glayer2_weights) + glayer2_biases)\n",
    "    #    return tf.reshape(hidden2,[-1,image_size, image_size,1])\n",
    "    \n",
    "    def deconv_generator_model(noise):\n",
    "        square_noise = tf.reshape(noise, [-1,8,8,1])\n",
    "        num_examples = tf.shape(square_noise)[0]\n",
    "        with tf.name_scope(\"generator\"):\n",
    "            fm1 = tf.nn.conv2d_transpose(value=square_noise, #[10,8,8,1]\n",
    "                                         filter=deconv_filter_1, #[5,5,64,1]\n",
    "                                         output_shape=[num_examples,12,12,256],\n",
    "                                         strides=[1,1,1,1],\n",
    "                                         padding='VALID', name=\"fm1\")\n",
    "            layer1 = tf.nn.relu(tf.nn.bias_add(fm1,deconv_bias_1), name=\"layer1\")\n",
    "            print(\"layer1 shape= {0}\".format(layer1.get_shape()))\n",
    "            fm2 = tf.nn.conv2d_transpose(value=layer1,\n",
    "                                         filter=deconv_filter_2,\n",
    "                                         output_shape=[num_examples,19,19,64],\n",
    "                                         strides=[1,1,1,1],\n",
    "                                         padding='VALID', name=\"fm2\")\n",
    "            print(\"deconv bias2 shape= {0}\".format(deconv_bias_2.get_shape()))\n",
    "            layer2 = tf.nn.relu(tf.nn.bias_add(fm2,deconv_bias_2), name=\"layer2\")\n",
    "            print(\"layer2 shape= {0}\".format(layer2.get_shape()))\n",
    "            fm3 = tf.nn.conv2d_transpose(value=layer2,\n",
    "                                         filter=deconv_filter_3,\n",
    "                                         output_shape=[num_examples,28,28,1],\n",
    "                                         strides=[1,1,1,1],\n",
    "                                         padding='VALID', name=\"fm3\")\n",
    "            layer3 = tf.nn.relu(tf.nn.bias_add(fm3,deconv_bias_3),name=\"layer3\")\n",
    "            print(\"layer3 shape= {0}\".format(layer3.get_shape()))\n",
    "            output = tf.reshape(layer3,[-1,image_size, image_size,1])\n",
    "            print(\"output shape= {0}\".format(output.get_shape()))\n",
    "        return output\n",
    "\n",
    "    #Generator Loss\n",
    "    sample_points = tf.constant(np.random.uniform(0,1,(batch_size,random_vector_size)).astype(np.float32))\n",
    "    test_image = deconv_generator_model(sample_points)\n",
    "    generated_image = deconv_generator_model(tf_train_random)\n",
    "    \n",
    "    generator_logits = classifier_model(generated_image)\n",
    "    generator_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(generator_logits, tf.ones([batch_size,1])))\n",
    "\n",
    "    # Generator Optimizer\n",
    "    generator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\")\n",
    "    generator_learnrate = tf.train.exponential_decay(0.05, global_step,\n",
    "                                           1000, 0.96, staircase=True)\n",
    "    generator_optimizer = tf.train.AdagradOptimizer(generator_learnrate).minimize(generator_loss, var_list=generator_variables, global_step=global_step)  \n",
    "\n",
    "    # Discriminator Loss\n",
    "    classifier_real_logits = classifier_model(tf_train_dataset)\n",
    "    classifier_real_loss = tf.reduce_mean(\n",
    "      tf.nn.sigmoid_cross_entropy_with_logits(classifier_real_logits, tf.ones([batch_size,1])))\n",
    "\n",
    "    classifier_fake_logits = classifier_model(generated_image)\n",
    "    classifier_fake_loss = tf.reduce_mean(\n",
    "      tf.nn.sigmoid_cross_entropy_with_logits(classifier_fake_logits, tf.zeros([batch_size,1])))\n",
    "\n",
    "    classifier_loss = classifier_real_loss + classifier_fake_loss\n",
    "\n",
    "    # Discriminator Optimizer.\n",
    "    classifier_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "    classifier_learnrate = tf.train.exponential_decay(0.05, global_step,\n",
    "                                           1000, 0.96, staircase=True)\n",
    "    classifier_optimizer = tf.train.AdagradOptimizer(classifier_learnrate).minimize(classifier_loss, var_list=classifier_variables)\n",
    "    \n",
    "    # Log variables\n",
    "    for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\"):\n",
    "        variable_summaries(var, var.name)\n",
    "    for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\"):\n",
    "        variable_summaries(var, var.name)\n",
    "\n",
    "    tf.image_summary(deconv_filter_1.name,deconv_filter_1)\n",
    "    merged = tf.merge_all_summaries()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 70000\n",
    "training_thresh = 0.4\n",
    "updating = 'discriminator'\n",
    "step = 0\n",
    "updated_generator=True\n",
    "l1,l2,l3 = .5, .5, .5\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    train_writer = tf.train.SummaryWriter('/tmp/train', session.graph)\n",
    "    print('Initialized')\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # prepare batch of training data\n",
    "        batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "        batch_data = batch_data.reshape([-1,28,28,1])\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_random: np.random.normal(0,1.0/np.sqrt(64),(batch_size,random_vector_size)).astype(np.float32)}\n",
    "\n",
    "        # decide if we should change which model to update\n",
    "        if float(l3)<training_thresh and updating=='generator':\n",
    "            print(\"Updating discriminator...\")\n",
    "            updating='discriminator'\n",
    "        elif float(l2+l1)<training_thresh and updating=='discriminator':\n",
    "            print(\"Updating generator...\")\n",
    "            updating='generator'\n",
    "        \n",
    "        if updating == 'generator':\n",
    "            updated_generator=True\n",
    "\n",
    "        # update model\n",
    "        #if updating=='discriminator':\n",
    "        #    _ = session.run([classifier_optimizer], feed_dict=feed_dict)\n",
    "        #elif(updating=='generator'):\n",
    "        #    _ = session.run([generator_optimizer], feed_dict=feed_dict)\n",
    "        #if l3<l1+l2:\n",
    "        _ = session.run([classifier_optimizer], feed_dict=feed_dict)\n",
    "        #else:\n",
    "        summary, _, gs = session.run([merged, generator_optimizer, global_step], feed_dict=feed_dict)\n",
    "        updated_generator=True\n",
    "        train_writer.add_summary(summary, gs)\n",
    "        #    _ = session.run([generator_optimizer], feed_dict=feed_dict)\n",
    "        #    updated_generator=True\n",
    "\n",
    "        if (step % 100 == 0):\n",
    "            # log/debug    \n",
    "            images, l3,l1, l2 = session.run([generated_image, generator_loss, classifier_real_loss, classifier_fake_loss], feed_dict=feed_dict)\n",
    "            print(\"Step {0}\".format(step))\n",
    "            print(\"Real and Fake loss: {0}\".format([l1,l2]))\n",
    "            print(\"Generator Loss {0}\".format(l3))\n",
    "            clr, glr = session.run([classifier_learnrate,generator_learnrate])\n",
    "            print (\"Classifier learn rate: {0}, Generator learn rate: {1}\".format(clr,glr))\n",
    "            \n",
    "            if updated_generator:\n",
    "                plot_images(session,test_image)\n",
    "                updated_generator=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
