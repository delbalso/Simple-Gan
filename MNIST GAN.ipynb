{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tm2CQN_Cpwj0"
   },
   "source": [
    "# Basic Generative Adversarial Network to generate MNIST images\n",
    "\n",
    "This code creates two networks: a generator and a discriminator. The discriminator tries to determine if it's input is coming from real MNIST data or not and the generator tries to fool the discriminator. The generator (which is the network that is making the images seen below) never actually sees data from the training set, it just connects to the discriminator and has a loss function based on the discriminator's output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import range\n",
    "import pylab as pl\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.rcParams[\"figure.figsize\"] = 15,15\n",
    "\n",
    "def plot_images(session,image_graph):\n",
    "    images = session.run([image_graph])\n",
    "    images = np.squeeze(images)\n",
    "    edge_size = np.sqrt(images.shape[0])\n",
    "    new_row = []\n",
    "    for i in xrange(images.shape[0]):\n",
    "        new_row.append(images[i])\n",
    "        if (i + 1) % edge_size == 0:\n",
    "            if i < edge_size: #full_image doesn't exist yet\n",
    "                full_image = np.hstack(new_row)\n",
    "            else:\n",
    "                full_image = np.vstack([full_image,np.hstack(new_row)])\n",
    "            new_row = []\n",
    "    pl.imshow(full_image,cmap=\"gray\")\n",
    "    display.display(pl.gcf())\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('/tmp/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.scalar_summary('mean/' + name, mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "    tf.scalar_summary('sttdev/' + name, stddev)\n",
    "    tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "    tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "    tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "STDOUT = 1\n",
    "STDERR = 2\n",
    "\n",
    "class FDRedirector(object):\n",
    "    \"\"\" Class to redirect output (stdout or stderr) at the OS level using\n",
    "        file descriptors.\n",
    "    \"\"\" \n",
    "\n",
    "    def __init__(self, fd=STDOUT):\n",
    "        \"\"\" fd is the file descriptor of the outpout you want to capture.\n",
    "            It can be STDOUT or STERR.\n",
    "        \"\"\"\n",
    "        self.fd = fd\n",
    "        self.started = False\n",
    "        self.piper = None\n",
    "        self.pipew = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\" Setup the redirection.\n",
    "        \"\"\"\n",
    "        if not self.started:\n",
    "            self.oldhandle = os.dup(self.fd)\n",
    "            self.piper, self.pipew = os.pipe()\n",
    "            os.dup2(self.pipew, self.fd)\n",
    "            os.close(self.pipew)\n",
    "\n",
    "            self.started = True\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\" Flush the captured output, similar to the flush method of any\n",
    "        stream.\n",
    "        \"\"\"\n",
    "        if self.fd == STDOUT:\n",
    "            sys.stdout.flush()\n",
    "        elif self.fd == STDERR:\n",
    "            sys.stderr.flush()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\" Unset the redirection and return the captured output. \n",
    "        \"\"\"\n",
    "        if self.started:\n",
    "            self.flush()\n",
    "            os.dup2(self.oldhandle, self.fd)\n",
    "            os.close(self.oldhandle)\n",
    "            f = os.fdopen(self.piper, 'r')\n",
    "            output = f.read()\n",
    "            f.close()\n",
    "\n",
    "            self.started = False\n",
    "            return output\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def getvalue(self):\n",
    "        \"\"\" Return the output captured since the last getvalue, or the\n",
    "        start of the redirection.\n",
    "        \"\"\"\n",
    "        output = self.stop()\n",
    "        self.start()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maxpool(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "def conv (input_data, shape, name=\"conv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable(\"w\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b = tf.get_variable(\"b\", [shape[3]], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        return tf.nn.conv2d(input_data, w, [1, 2, 2, 1], padding='SAME')+b\n",
    "\n",
    "\n",
    "def fully_connected(input_data, shape, name=\"fully_connected\"):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable(\"w\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b = tf.get_variable(\"b\", [shape[1]], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    return tf.matmul(input_data, w) + b\n",
    "\n",
    "def discriminator(data, reuse=True):\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        channels = 32\n",
    "        d_l1 = maxpool(conv(data, shape=[3, 3, num_channels, channels], name=\"layer1\"))\n",
    "        d_l2 = conv(tf.nn.relu(d_l1), shape=[3, 3, channels, channels/2], name=\"layer2\")\n",
    "        d_l3 = conv(tf.nn.relu(d_l2), shape=[3, 3, channels/2, channels/4], name=\"layer3\")\n",
    "        d_l3_reshaped = tf.reshape(d_l3, [tf.shape(d_l3)[0],2*2*8])\n",
    "        fm3_shape = 2*2*channels/4\n",
    "        d_l4 = fully_connected(tf.nn.relu(d_l3_reshaped), shape=[fm3_shape,fm3_shape/2], name=\"layer4\")\n",
    "        d_l5 = fully_connected(d_l4, shape=[fm3_shape/2,1], name=\"layer5\")\n",
    "        print (\"d_l5 {0}\".format(d_l5.get_shape()))\n",
    "        #output = tf.matmul(reshape,w) + b\n",
    "        return tf.Print(tf.nn.sigmoid(d_l5),[tf.shape(d_l5)], first_n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_hidden = 64\n",
    "\n",
    "image_size = 28\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "random_vector_size = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels)) #images\n",
    "    tf_train_random = tf.placeholder(tf.float32, shape=(batch_size, random_vector_size))\n",
    "\n",
    "    # Variables for generator\n",
    "    hidden_layer1_size = 50\n",
    "    with tf.name_scope(\"normal_generator\"):\n",
    "        glayer1_weights = tf.Variable(tf.truncated_normal([random_vector_size,hidden_layer1_size], stddev=0.1), name=\"w1\")\n",
    "        glayer2_weights = tf.Variable(tf.truncated_normal([hidden_layer1_size,image_size * image_size], stddev=0.1), name=\"w2\")\n",
    "        glayer1_biases = tf.Variable(tf.zeros([hidden_layer1_size]), name=\"b1\")\n",
    "        glayer2_biases = tf.Variable(tf.zeros([image_size * image_size]), name=\"b2\")\n",
    "    \n",
    "    # Variables for deconv generator\n",
    "    with tf.name_scope(\"generator\"):\n",
    "        deconv_filter_1 = tf.Variable(tf.truncated_normal([5,5,256,1], stddev=0.1), name=\"filters/1\")\n",
    "        deconv_filter_2 = tf.Variable(tf.truncated_normal([8,8,64,256], stddev=0.1), name=\"filters/2\")\n",
    "        deconv_filter_3 = tf.Variable(tf.truncated_normal([10,10,1,64], stddev=0.1), name=\"filters/3\")\n",
    "        deconv_offset_1 = tf.Variable(tf.truncated_normal([256], stddev=0.1), name=\"offset1\")\n",
    "        deconv_offset_2 = tf.Variable(tf.truncated_normal([64], stddev=0.1), name=\"offset2\")\n",
    "        deconv_offset_3 = tf.Variable(tf.truncated_normal([1], stddev=0.1), name=\"offset3\")\n",
    "        deconv_scale_1 = tf.Variable(tf.truncated_normal([256], stddev=0.1), name=\"scale1\")\n",
    "        deconv_scale_2 = tf.Variable(tf.truncated_normal([64], stddev=0.1), name=\"scale2\")\n",
    "        deconv_scale_3 = tf.Variable(tf.truncated_normal([1], stddev=0.1), name=\"scale3\")\n",
    "\n",
    "  \n",
    "    def batch_norm(inputs, scale, offset):\n",
    "        mean, variance = tf.nn.moments(inputs,axes=[0,1,2])\n",
    "        return tf.nn.batch_normalization(inputs, mean, variance, offset, scale, variance_epsilon=1e-5)\n",
    "    \n",
    "    def deconv_generator_model(noise):\n",
    "        square_noise = tf.reshape(noise, [-1,8,8,1])\n",
    "        num_examples = tf.shape(square_noise)[0]\n",
    "        with tf.name_scope(\"generator\"):\n",
    "            fm1 = tf.nn.conv2d_transpose(value=square_noise, #[10,8,8,1]\n",
    "                                         filter=deconv_filter_1, #[5,5,64,1]\n",
    "                                         output_shape=[num_examples,12,12,256],\n",
    "                                         strides=[1,1,1,1],\n",
    "                                         padding='VALID', name=\"fm1\")\n",
    "            layer1 = tf.nn.relu(batch_norm(fm1,deconv_scale_1, deconv_offset_1))\n",
    "            print(\"layer1 shape= {0}\".format(layer1.get_shape()))\n",
    "            fm2 = tf.nn.conv2d_transpose(value=layer1,\n",
    "                                         filter=deconv_filter_2,\n",
    "                                         output_shape=[num_examples,19,19,64],\n",
    "                                         strides=[1,1,1,1],\n",
    "                                         padding='VALID', name=\"fm2\")\n",
    "            #print(\"deconv bias2 shape= {0}\".format(deconv_bias_2.get_shape()))\n",
    "            layer2 = tf.nn.relu(batch_norm(fm2,deconv_scale_2, deconv_offset_2))\n",
    "            print(\"layer2 shape= {0}\".format(layer2.get_shape()))\n",
    "            fm3 = tf.nn.conv2d_transpose(value=layer2,\n",
    "                                         filter=deconv_filter_3,\n",
    "                                         output_shape=[num_examples,28,28,1],\n",
    "                                         strides=[1,1,1,1],\n",
    "                                         padding='VALID', name=\"fm3\")\n",
    "            layer3 = tf.nn.relu(batch_norm(fm3,deconv_scale_3, deconv_offset_3))\n",
    "            print(\"layer3 shape= {0}\".format(layer3.get_shape()))\n",
    "            output = tf.reshape(layer3,[-1,image_size, image_size,1])\n",
    "            print(\"output shape= {0}\".format(output.get_shape()))\n",
    "        return output\n",
    "\n",
    "    #Generator Loss\n",
    "    sample_points = tf.constant(np.random.uniform(0,1,(batch_size,random_vector_size)).astype(np.float32))\n",
    "    debug_image = deconv_generator_model(sample_points)\n",
    "    generated_image = deconv_generator_model(tf_train_random)\n",
    "    \n",
    "    generator_logits = discriminator(generated_image, reuse=False)\n",
    "    generator_loss = tf.nn.l2_loss(generator_logits- tf.ones([batch_size,1]))\n",
    "\n",
    "    # Generator Optimizer\n",
    "    generator_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\")\n",
    "    generator_learnrate = tf.train.exponential_decay(0.05, global_step, 1000, 0.96, staircase=True)\n",
    "    generator_optimizer = tf.train.AdagradOptimizer(generator_learnrate).minimize(generator_loss, var_list=generator_variables, global_step=global_step)  \n",
    "\n",
    "    # Discriminator Loss\n",
    "    classifier_real_logits = discriminator(tf_train_dataset)\n",
    "    print(tf_train_dataset.get_shape())\n",
    "    print(classifier_real_logits.get_shape())\n",
    "    classifier_real_loss = tf.nn.l2_loss(classifier_real_logits - tf.ones([batch_size,1]))\n",
    "\n",
    "    classifier_fake_logits = generator_logits\n",
    "    classifier_fake_loss = tf.nn.l2_loss(classifier_fake_logits - tf.zeros([batch_size,1]))\n",
    "\n",
    "    classifier_loss = classifier_real_loss + classifier_fake_loss\n",
    "\n",
    "    # Discriminator Optimizer.\n",
    "    classifier_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "    classifier_learnrate = tf.train.exponential_decay(0.05, global_step, 1000, 0.96, staircase=True)\n",
    "    classifier_optimizer = tf.train.AdagradOptimizer(classifier_learnrate).minimize(classifier_loss, var_list=classifier_variables, global_step=global_step)\n",
    "    d_opt = tf.train.AdagradOptimizer(classifier_learnrate)\n",
    "    d_gradients = d_opt.compute_gradients(classifier_loss, classifier_variables)\n",
    "    d_apply = d_opt.apply_gradients(d_gradients, global_step=global_step)\n",
    "    \n",
    "    # Log variables\n",
    "    for var in classifier_variables:\n",
    "        print(var.name)\n",
    "        variable_summaries(var, var.name)\n",
    "    for var in generator_variables:\n",
    "        variable_summaries(var, var.name)\n",
    "    variable_summaries(classifier_real_logits, \"discriminator/real_logits\")\n",
    "    variable_summaries(classifier_fake_logits, \"discriminator/fake_logits\")\n",
    "    #variable_summaries(tf.nn.sigmoid_cross_entropy_with_logits(classifier_real_logits, tf.ones([batch_size,1])), \"discriminator/cross_entropy_real_logits\")\n",
    "    #variable_summaries(tf.nn.sigmoid_cross_entropy_with_logits(classifier_fake_logits, tf.zeros([batch_size,1])), \"discriminator/cross_entropy_fake_logits\")\n",
    "    variable_summaries(generator_logits, \"generator/logits\")\n",
    "        \n",
    "        \n",
    "    check = tf.add_check_numerics_ops()\n",
    "\n",
    "    tf.image_summary(deconv_filter_1.name,deconv_filter_1)\n",
    "    merged = tf.merge_all_summaries()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 70000\n",
    "training_thresh = 0.4\n",
    "updating = 'discriminator'\n",
    "step = 0\n",
    "updated_generator=True\n",
    "l1,l2,l3 = .5, .5, .5\n",
    "redirect=FDRedirector(STDERR)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    train_writer = tf.train.SummaryWriter('/tmp/train', session.graph)\n",
    "    print('Initialized')\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # prepare batch of training data\n",
    "        batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "        batch_data = batch_data.reshape([-1,28,28,1])\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_random: np.random.normal(0,1.0/np.sqrt(64),(batch_size,random_vector_size)).astype(np.float32)}\n",
    "\n",
    "        # decide if we should change which model to update\n",
    "        if float(l3)<training_thresh and updating=='generator':\n",
    "            print(\"Updating discriminator...\")\n",
    "            updating='discriminator'\n",
    "        elif float(l2+l1)<training_thresh and updating=='discriminator':\n",
    "            print(\"Updating generator...\")\n",
    "            updating='generator'\n",
    "\n",
    "        # update model\n",
    "        #if updating=='discriminator':\n",
    "        #    _ = session.run([classifier_optimizer], feed_dict=feed_dict)\n",
    "        #elif(updating=='generator'):\n",
    "        #    _ = session.run([generator_optimizer], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        #if l3<l1+l2:\n",
    "        #redirect.start()\n",
    "        _ = session.run([check], feed_dict=feed_dict)\n",
    "        summary, grad, _,gs = session.run([merged, d_gradients, d_apply, global_step], feed_dict=feed_dict)\n",
    "        #classifier_real_logits.eval(feed_dict=feed_dict)\n",
    "        #summary, _ = session.run([merged, generator_optimizer], feed_dict=feed_dict)\n",
    "        _ = session.run([generator_optimizer], feed_dict=feed_dict)\n",
    "        _ = session.run([generator_optimizer], feed_dict=feed_dict)\n",
    "        _ = session.run([generator_optimizer], feed_dict=feed_dict)\n",
    "        #print (redirect.stop())\n",
    "        #else:\n",
    "        #    _ = session.run([generator_optimizer], feed_dict=feed_dict)\n",
    "        #    _ = session.run([generator_optimizer], feed_dict=feed_dict)\n",
    "        #summary, _, gs = session.run([merged, generator_optimizer, global_step], feed_dict=feed_dict)\n",
    "\n",
    "        #raise\n",
    "        updated_generator=True\n",
    "        train_writer.add_summary(summary, gs)\n",
    "        #    _ = session.run([generator_optimizer], feed_dict=feed_dict)\n",
    "        #    updated_generator=True\n",
    "\n",
    "        if (step % 100 == 0):\n",
    "            #if step >1000:\n",
    "            #    raise\n",
    "            #for var in classifier_variables:\n",
    "            #    print(\"Name: {0}, Shape: {1}\".format(var.name,tf.shape(var).eval()))\n",
    "            assert (len(grad)==len(classifier_variables))\n",
    "            for i in xrange(len(classifier_variables)):\n",
    "                g = grad[i][0]\n",
    "                print (\"{0}'s gradients have {1} ({2}%) zeros and mean {3}\".format(classifier_variables[i].name,g.size-np.count_nonzero(g), (g.size-np.count_nonzero(g)+0.0)/g.size*100, np.mean(g)))\n",
    "            #for gradient, var in grad:\n",
    "            #    print(\"Variable: {0}\\n------gradient: {1}\".format(var, gradient))\n",
    "            \n",
    "            \n",
    "            # log/debug    \n",
    "            images, l3,l1, l2 = session.run([generated_image, generator_loss, classifier_real_loss, classifier_fake_loss], feed_dict=feed_dict)\n",
    "            print(\"Step {0}\".format(step))\n",
    "            print(\"Classifier loss: {0}, Real: {1}, Fake: {2}\".format(l1+l2, l1,l2))\n",
    "            print(\"Generator Loss: {0}\".format(l3))\n",
    "            clr, glr = session.run([classifier_learnrate,generator_learnrate])\n",
    "            print (\"Classifier learn rate: {0}, Generator learn rate: {1}\".format(clr,glr))\n",
    "            \n",
    "            if updated_generator:\n",
    "                plot_images(session,debug_image)\n",
    "                updated_generator=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
